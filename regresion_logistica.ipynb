{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta libreta vamos a desarrollar los algoritmos de regresión logística, y vamos a aplicar los métodos a dos conjuntos de datos, uno donde se aplica directamente la regresión logística y otro donde se ejemplifica el uso de la regularización y los clasificadores polinomiales.\n",
    "\n",
    "Igualmente vamos a probar el uso del descenso de gradiente, y el uso de herramientas de optimización que provienen de los paquetes de `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image  # Esto es para desplegar imágenes en la libreta\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Función logística, función de costo y gradiente de la función de costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función logística está dada por \n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "\n",
    "la cual tambien se conoce como función *probit* (probabilidad de un bit). La cual es importante que podamos calcular en forma vectorial. Si bien el calculo es de una sola linea, el uso de estas funciones auxiliares facilitan la legibilidad del código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 1: Desarrolla la función logística, la cual se calcule para todos los elementos de un ndarray (20 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Calcula la función logística para cada elemento de z\n",
    "    \n",
    "    @param z: un ndarray\n",
    "    @return: un ndarray de las mismas dimensiones que z\n",
    "    \"\"\"\n",
    "    # Introduce código aqui (una linea de código)\n",
    "    #---------------------------------------------------\n",
    "    return 1/(1+np.exp(-z))\n",
    "    #---------------------------------------------------\n",
    "    \n",
    "# prueba que efectivamente funciona la función implementada\n",
    "# si el assert es falso regresa un error de aserción (el testunit de los pobres)\n",
    "assert (np.abs(logistica(np.array([-1, 0, 1])) - np.array([ 0.26894142, 0.5, 0.73105858]))).sum() < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función vamos a graficar la función logística en el intervalo [-5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEtCAYAAABOGLRfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xb5e75c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 100)\n",
    "with plt.xkcd():\n",
    "    plt.plot( z, logistica(z))\n",
    "    plt.title(u'Función logística', fontsize=20)\n",
    "    plt.xlabel(r'$z$', fontsize=20)\n",
    "    plt.ylabel(r'$\\frac{1}{1 + \\exp(-z)}$', fontsize=26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez establecida la función logística, vamos a implementar la función de costo *sin regularizar* para la regresión logística, la cual está dada por\n",
    "\n",
    "$$\n",
    "J(\\omega) = -\\frac{1}{T} \\sum_{i=1}^T \\left[ y^{(i)}\\log(h_\\omega(x^{(i)}) + (1 - y^{(i)})\\log(1 - h_\\omega(x^{(i)}))\\right],\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "h_\\omega(x^{(i)}) = g(\\omega^T x_e^{(i)}),\n",
    "$$\n",
    "\n",
    "las cuales fueron ecuaciones revisadas en clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2: Implementa la función de costo para un conjunto de aprendizaje (20 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def costo(w, x, y):\n",
    "    \"\"\"\n",
    "    Calcula el costo de una w dada para el conjunto dee entrenamiento dado por y y x\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1) \n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \n",
    "    @return: un flotante con el costo\n",
    "    \n",
    "    \"\"\" \n",
    "   \n",
    "    #T= num de renglones de la matriz X\n",
    "    T = x.shape[0]\n",
    "    \n",
    "    #L sera un vector que será la logistica de cada elemento del vector que resulta al multiplicar el producto punto de la\n",
    "    #matriz x por el vector w\n",
    "    L=logistica(x.dot(w))\n",
    "    #LogL será un vector que se calculará obteniendo la funcion logistica de cada elemento del vector L\n",
    "    LogL=np.log(L)\n",
    "    #calculamos el producto punto del vector transpuesta de -Y y lo multiplicamos por el vector LogL y el resultado será un\n",
    "    #valor real que llamaremos \"C\"\n",
    "    c=(-y.T.dot(LogL))\n",
    "    #formaremos un nuevo vector llamada nueva_y que se calcula restandole al 1 cada valor del vector y\n",
    "    nueva_y=1-y\n",
    "    #tenemos un nuevo vector llamado a que será el vector transpuesto del vector nueva_y\n",
    "    a=nueva_y.T\n",
    "    #formamos una nueva matriz en donde al numero 1 le quitamos cada cada elemento de la matriz L\n",
    "    nueva_L=1-L\n",
    "    #calculamos un nuevo vector b obteniendo el logaritmo de cada elemento del vector nueva_L\n",
    "    b=np.log(nueva_L)\n",
    "    #calculamos el producto punto del vector a por el vector b y lo guardamos en d, en donde d es un numero real\n",
    "    d=a.dot(b)\n",
    "    #por ultimo calculamos el costo de la siguiente manera:\n",
    "    costo=(c-d)/T\n",
    "    return costo    \n",
    "    \n",
    "   \n",
    "\n",
    "    \n",
    "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n",
    "w = np.ones((2,1))\n",
    "\n",
    "x = np.array([[1, 10],\n",
    "              [1, -5]])\n",
    "\n",
    "y1 = np.array([[1],\n",
    "               [0]])\n",
    "\n",
    "y2 = np.array([[0],\n",
    "               [1]])\n",
    "\n",
    "y3 = np.array([[0],\n",
    "               [0]])\n",
    "\n",
    "y4 = np.array([[1],\n",
    "               [1]])\n",
    "\n",
    "#print abs(costo(w, x, y1) - 0.01)\n",
    "#print abs(costo(w, x, y2) - 0.01)\n",
    "#print abs(costo(w, x, y3) - 0.01)\n",
    "#print abs(costo(w, x, y4) - 0.01)\n",
    "\n",
    "#print costo(w, x, y1)\n",
    "#assert abs(costo(w, x, y1) - 0.01)< 1e-2\n",
    "#assert abs(costo(w, x, y2) - 7.5) < 1e-2\n",
    "#assert abs(costo(w, x, y3) - 5.5) < 1e-2\n",
    "#assert abs(costo(w, x, y4) - 2.0) < 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, para poder implementar las funciones de aprendizaje, vamos a implementar el gradiente de la función de costo. El gradiente de la función de costo respecto a $\\omega$ es (como lo vimos en clase) el siguiente:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\omega)}{\\partial \\omega_j} = -\\frac{1}{T} \\sum_{i=1}^T (y^{(i)} - h_\\omega(x^{(i)})x_j^{(i)} \n",
    "$$\n",
    "\n",
    "y a partir de las ecuaciones individuales de puede obtener $\\nabla J(\\omega)$, la cual no la vamos a escribir en la libreta para que revisen en sus notas como se puede resolver este problema en forma matricial. Si bien para el descenso de gradiente podemos utilizar directamente el gradiente negado, al implementar métodos de optimización avanzados, necesitamos que el gradiente sea efectivamente el gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 3: Implementa (con operaciones matriciales) la función del gradiente (20 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object has no attribute '__getitem__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-b8392e30f75d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m                [1]])\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.00898475\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgradiente\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m7.45495097\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgradiente\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4.95495097\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mgradiente\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object has no attribute '__getitem__'"
     ]
    }
   ],
   "source": [
    "def gradiente(w, x, y):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente del costo de la regrasión logística, para una theta, conociendo un conjunto de aprendizaje.\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1) \n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \n",
    "    @return: un ndarray de mismas dimensiones que w\n",
    "    \n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n",
    "w = np.ones((2, 1))\n",
    "\n",
    "x = np.array([[1, 10],\n",
    "              [1, -5]])\n",
    "\n",
    "y1 = np.array([[1],\n",
    "               [0]])\n",
    "\n",
    "y2 = np.array([[0],\n",
    "               [1]])\n",
    "\n",
    "y3 = np.array([[0],\n",
    "               [0]])\n",
    "\n",
    "y4 = np.array([[1],\n",
    "               [1]])\n",
    "\n",
    "assert abs(0.00898475 - gradiente(w, x, y1)[0]) < 1e-4\n",
    "assert abs(7.45495097 - gradiente(w, x, y2)[1]) < 1e-4 \n",
    "assert abs(4.95495097 - gradiente(w, x, y3)[1]) < 1e-4 \n",
    "assert abs(-0.49101525 - gradiente(w, x, y4)[0]) < 1e-4 \n",
    "#exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descenso de gradiente y el método BGFS para regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a desarrollar las funciones necesarias para realizar el entrenamiento y encontrar la mejor $\\omega$ de acuero a la función de costos y un conjunto de datos de aprendizaje.\n",
    "\n",
    "Para este problema, vamos a utilizar una base de datos sintética proveniente del curso de Andrew Ng que se encuentra en coursera. Supongamos que pertenecemos al departamente de servicios escolares de la UNISON y vamos a modificar el procedimiento de admisión, y en lugar de utilizar un solo exámen (EXCOBA) y la información del cardex de la preparatoria, hemos decidido aplicar dos exámenes, uno sicométrico y otro de habilidades estudiantiles. Dichos exámenes se han aplicado el último año aunque no fueron utilizados como criterio. Así, tenemos un historial entre estudiantes aceptados y resultados de los dos exámenes. El objetivo es hacer un método de regresión que nos permita hacer la admisión a la UNISON tomando en cuenta únicamente los dos exámenes y simplificar el proceso. *Recuerda que esto no es verdad, es solo un ejercicio*.\n",
    "\n",
    "Bien, los datos se encuentran en el archivo `admision.txt` el cual se encuentra en formato `cvs` (osea los valores de las columnas separados por comas. Vamos a utilizar el mismo método para cargar datos que el que utilizamos para regresión lineal, y vamos a graficar la información para entender un poco los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos = np.loadtxt('admision.txt', comments='%', delimiter=',')\n",
    "\n",
    "x, y = datos[:,0:-1], datos[:,-1:] \n",
    "x = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='aceptados') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazados')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n",
    "#sys.exit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vistos los datos un clasificador lineal podría ser una buena solución. \n",
    "\n",
    "Ahora vamos a implementar el método de descenso de gradiente, casi de la misma manera que lo implementamos para regresión lineal (por lotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4: Implementa el descenso de gradiente para el problema de regresión logística en modo batch (20 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def descenso_rl_lotes(x, y, alpha, epsilon=1e-4, max_iter=int(1e4), costos=False):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente por lotes para resolver el problema de regresión logística con un conjunto de aprendizaje\n",
    "\n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    @param alpha: Un flotante (típicamente pequeño) con la tasa de aprendizaje\n",
    "    @param epsilon: Un flotante pequeño como criterio de paro. Por default 1e-4\n",
    "    @param max_iter: Máximo numero de iteraciones. Por default 1e4\n",
    "    @param costos: Un booleano para saber si calculamos el historial de costos o no\n",
    "    \n",
    "    @return: w, costo_hist donde costo es ndarray de dimensión (n + 1, 1) y costo_hist es un\n",
    "             ndarray de dimensión (max_iter,) con el costo en cada iteración si costos == True, si no\n",
    "             regresa None\n",
    "             \n",
    "    \"\"\"\n",
    "    \n",
    "    T, n = x.shape[0], x.shape[1] - 1\n",
    "    \n",
    "    w = np.zeros((n + 1, 1))\n",
    "    costo_hist = np.zeros(max_iter) if costos else None\n",
    "    \n",
    "    for iter in xrange(max_iter):\n",
    "    #--------------------------------------------------------------\n",
    "        # Agregar aqui tu código\n",
    "        #\n",
    "        # Recuerda utilizar las funciones que ya has desarrollado\n",
    "        \n",
    "        \n",
    "            \n",
    "    return w, costo_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función de aprendizaje, vamos a aplicarla a nuestro problema de admisión. Primero recuerda que tienes que hacer una exploración para encontrar el mejor valor de $\\alpha$. Así que utiliza el código de abajo para ajustar $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 1e-4\n",
    "mi = 50\n",
    "_, costo_hist = descenso_rl_lotes(x, y, alpha, epsilon=1e-4, max_iter=mi, costos=True)\n",
    "\n",
    "plt.plot(np.arange(mi), costo_hist)\n",
    "plt.title(r'Evolucion del costo en las primeras iteraciones con $\\alpha$ = ' + str(alpha))\n",
    "plt.xlabel('iteraciones')\n",
    "plt.ylabel('costo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez encontrada la mejor $\\alpha$, entonces podemos calcular $\\omega$ (esto va a tardar bastante), recuerda que el costo final debe de ser lo más cercano a 0 posible, así que agrega cuantas iteraciones sean necesarias: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_lotes, _ = descenso_rl_lotes(x, y, alpha, max_iter = int(1e6))\n",
    "print w_lotes\n",
    "print costo(w_lotes, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante ver como el descenso de gradiente no es eficiente en este tipo de problemas, a pesar de ser problemas de optimización convexos.\n",
    "\n",
    "Bueno, este método nos devuelve $\\omega$, pero esto no es suficiente para ecir que tenemos un clasificador, ya que un método de clasificación se compone de dos métodos, uno para **aprender** y otro para **predecir**. Recuerda que para realizar la predicción con el método de regrasión lineal y el criterio MAP, no es necesario calcular la regrasión logística completa (lo que hace un método de predicción más rápido)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5: Desarrolla una función de predicción (20 puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictor(w, x):\n",
    "    \"\"\"\n",
    "    Predice los valores de y_hat (que solo pueden ser 0 o 1), utilizando el criterio MAP.\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1)\n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "\n",
    "    @return: y_hat un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \"\"\"\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    \n",
    "    \n",
    "    #va a checar todo el vector de x.dot(w) si un elemento es mayor que 0 entonces se guardara 1 sino se guarda 0 en el\n",
    "    #vector y_estimada\n",
    "    dato=x.dot(w)\n",
    "    print \"dato:\",dato\n",
    "    \n",
    "    y_estimada = np.where(x.dot(w)>0,1,0)\n",
    "    return y_estimada \n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------------\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que tan bueno es este clasificador? ¿Es que implementamos bien el método?\n",
    "\n",
    "Vamos a contestar esto por partes. Primero, vamos a graficar los mismos datos pero vamos a agregar la superficie de separación, la cual en este caso sabemos que es una linea recta. Como sabemos el criterio para decidir si un punto pertenece a la clase 1 o cero es si el valor de $\\omega^T x_e^{(i)} \\ge 0$, por lo que la frontera entre la región donde se escoge una clase de otra se encuentra en:\n",
    "\n",
    "$$\n",
    "0 = \\omega_0 + \\omega_1 x_1  + \\omega_2 x_2,\n",
    "$$\n",
    "\n",
    "y despejando:\n",
    "\n",
    "$$\n",
    "x_2 = -\\frac{\\omega_0}{\\omega_2} -\\frac{\\omega_1}{\\omega_2}x_1\n",
    "$$\n",
    "\n",
    "son los pares $(x_1, x_2)$ de valores en la forntera. Al ser estos (en este caso) una linea recta solo necesitamos dos para graficar la superficie de separación. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1_frontera = np.array([20, 100]) #Los valores mínimo y máximo que tenemos en la gráfica de puntos\n",
    "x2_frontera = -(w_lotes[0] / w_lotes[2]) - (w_lotes[1] / w_lotes[2]) * x1_frontera\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='aceptados') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazados')\n",
    "plt.plot(x1_frontera, x2_frontera, 'm')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para que tengas una idea de lo que debería de salir, anexo una figura obtenida con el código que yo hice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='ejemplo_logistica_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, ya vimos que el método del descenso de gradiente, si bien es correcto y fácil de implementar, es bastante ineficiente en cuanto a tiempo (inclusive para un problema bastante simple, por lo que vamos a utilizar la función `minimize`que provee `scipy` la que vamos a utilizar con el algoritmo BFGS (explicado en clase en forma somera). Ejecuta la celda de abajo para revisar la documentación de `minimize` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "#minimize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, el método BFGS es el método por default. Los parámetros que hay que agregar son:\n",
    "\n",
    "* Una $\\omega$ inicial en `x0`\n",
    "* Una funcion de costo $J(\\omega)$ en `fun`\n",
    "* Una función de gradiente $\\nabla J(\\omega)$ en `jac`\n",
    "* Argumentos extras para `fun` y `jac` en forma de tupla\n",
    "* Opciones del método de optimización en `options` como un diccionario. Las opciones pueden ser:\n",
    "  * gtol: Valor de tolerancia en la variación de la norma del gradiente\n",
    "  * maxiter: Máximo numero de iteraciones\n",
    "  * disp: Si True, despliega la información sobre la convergencia del algoritmo\n",
    "\n",
    "La función regresa un objeto resultado `res`, del cual solo nos interesa `res.x`, el resultado de theta.\n",
    "\n",
    "Como $\\omega$ en este método debe de ser un ndim de una sola dimensión, y nosotros hemos estado usando $\\theta$ como un vector columna pues podemos modificar un poco estas funciones con unas nuevas (esto de hecho es posible hacerlo modificando las funciones originales, pero podría causar mas problemas de conceptos hacerlo). De hecho una vez funcionando, un buen científica de la computación (o un buen desarrollador) aplicaría una etapa de *refactoring*, esto es, modificar todo lo que obscurece y hace menos ineficiente, o menos estético el código.\n",
    "\n",
    "Veras que si lo corres varias veces, en muchas ocasiones vamos a tener que el resultado no se pudo lograr por falta de precisión en la función del gradiente. Esto es debido a la forma en que estamos calculando el gradiente, y a que si la solución inicial realiza una partición pésima del estado, entonces hay problemas para estimar el inverso del hessiano, por problemas de estabilidad numérica del método. Por esta razón se inicializa el algoritmo con un valor de $\\omega$ obtenido en forma aleatoria (valores pequeños).\n",
    "\n",
    "Lo interesante es que el algoritmo avisa cuando no hay convergencia, y vuando hay se obtiene en muy pocas iteraciones (19 en promedio)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w0 = 1e-2 * np.random.rand(x.shape[1])\n",
    "\n",
    "funcion = lambda w, x, y: costo(w.reshape(-1,1), x, y)\n",
    "jacobiano = lambda w, x, y: gradiente(w.reshape(-1,1), x, y).ravel()\n",
    "res = minimize(x0=w0,\n",
    "               fun=funcion,\n",
    "               jac=jacobiano,\n",
    "               args = (x, y),\n",
    "               method='BFGS',\n",
    "               options= {'maxiter': 400, 'disp': True})\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de obtener el resltado es utilizando un método tipo *simplex* que no necesita calcular ni el jacobiano, ni la inversión del hessiano. Que si bien requiere sensiblemente de más iteraciones, para un problema relativamente simple como este es suficiente (sin embargo para problemas de regresión lineal con muchos atributos, o redes neuronales, ya no será suficiente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w0 = np.zeros(x.shape[1])\n",
    "\n",
    "funcion = lambda w, x, y: costo(w.reshape(-1,1), x, y)\n",
    "jacobiano = lambda w, x, y: gradiente(w.reshape(-1,1), x, y).ravel()\n",
    "res = minimize(x0=w0,\n",
    "               fun=funcion,\n",
    "               args = (x, y),\n",
    "               method='Nelder-Mead',\n",
    "               options= {'maxiter': 400, 'disp': True})\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora veamos la partición del espacio con este método, la cual es ligeramente diferente a la obtenida con el descenso de gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_NM = res.x.reshape(-1,1)\n",
    "\n",
    "x1_frontera = np.array([20, 100]) #Los valores mínimo y máximo que tenemos en la gráfica de puntos\n",
    "x2_frontera = -(w_NM[0] / w_NM[2]) - (w_NM[1] / w_NM[2]) * x1_frontera\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='aceptados') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazados')\n",
    "plt.plot(x1_frontera, x2_frontera, 'm')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
